{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##load in libraries\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import ttest_ind\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, io\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import pairwise_distances as pdist\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from scipy.stats import entropy\n",
    "from tqdm import tqdm\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "import matplotlib as mpl\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.image as mpimg\n",
    "import ast\n",
    "from scipy.spatial.distance import jensenshannon as jsd\n",
    "from sklearn.metrics import auc\n",
    "import statsmodels.formula.api as smf\n",
    "from scipy.stats import rankdata\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "import warnings\n",
    "import random\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### set font properties for plots\n",
    "sns.set(font='Helvetica')\n",
    "sns.set_style('white')\n",
    "sns.set_context('paper')\n",
    "lfont = {'fontname':'Helvetica'}\n",
    "mpl.rcParams['pdf.fonttype'] = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_human_snp(this_sample_recog_df):\n",
    "    concepts=[]\n",
    "    ranks = []\n",
    "    cum_props=[]\n",
    "    abstraction =[]\n",
    "\n",
    "    for this_concept in human_response_vec_df.uniqueID.unique(): ## to maintain same order as response vec df\n",
    "        for this_abstraction in human_response_vec_df.abstraction.unique():\n",
    "\n",
    "            ds= this_sample_recog_df[(this_sample_recog_df['uniqueID']==this_concept) & (this_sample_recog_df['abstraction']==this_abstraction)]\n",
    "            ds = ds[ds.top1_rank!=0]\n",
    "            ranks.append(ds['top1_rank'].value_counts(normalize=True).sort_index().cumsum().index)\n",
    "            cum_props.append(ds['top1_rank'].value_counts(normalize=True).sort_index().cumsum().values)\n",
    "            concepts.append([this_concept]*ds['top1_rank'].value_counts(normalize=True).sort_index().cumsum().index.shape[0])\n",
    "            abstraction.append([this_abstraction]*ds['top1_rank'].value_counts(normalize=True).sort_index().cumsum().index.shape[0])\n",
    "\n",
    "    ranks = np.concatenate(ranks)\n",
    "    cum_props = np.concatenate(cum_props)\n",
    "    concepts = np.concatenate(concepts)\n",
    "    abstraction = np.concatenate(abstraction)\n",
    "\n",
    "    this_cum_prop_df = pd.DataFrame({'concept':concepts,'rank':ranks,'cum_prop':cum_props,'abstraction':abstraction})\n",
    "\n",
    "    this_cum_prop_df['rank'] = this_cum_prop_df['rank']/1854\n",
    "\n",
    "\n",
    "\n",
    "    this_auc_df = pd.DataFrame(columns = ['concept','auc'])\n",
    "    for this_concept in this_cum_prop_df.concept.unique():\n",
    "        for this_abstraction in this_cum_prop_df.abstraction.unique():\n",
    "            this_df = this_cum_prop_df[(this_cum_prop_df['concept']==this_concept) & (this_cum_prop_df['abstraction']==this_abstraction)]\n",
    "            this_df = pd.concat([this_df,pd.DataFrame({'concept':this_concept,'rank':1,'cum_prop':1,'abstraction':this_abstraction},index=[0])])\n",
    "            # return this_df\n",
    "            # this_auc= this_df.groupby('concept').apply(lambda x: auc(x['rank'],x['cum_prop']))\n",
    "            this_auc = auc(this_df['rank'],this_df['cum_prop'])\n",
    "            this_auc_df = pd.concat([this_auc_df,pd.DataFrame({'concept':this_concept,'auc':this_auc,'abstraction':this_abstraction},index=[0])])\n",
    "    return this_auc_df\n",
    "\n",
    "\n",
    "def compute_snp_EV(this_sample_df, this_sample_recog_df,this_model,this_auc_df):\n",
    "\n",
    "    concepts=[]\n",
    "    ranks = []\n",
    "    cum_props=[]\n",
    "    models=[]\n",
    "    abstraction =[]\n",
    "\n",
    "    for concept in this_sample_df.uniqueID.unique():\n",
    "\n",
    "        for this_abstraction in this_sample_df.abstraction.unique():\n",
    "            ds= this_sample_df[(this_sample_df['uniqueID']==concept) &(this_sample_df['abstraction']==this_abstraction)]\n",
    "            ds = ds[ds[f'top1_rank_{this_model}_late']!=0]\n",
    "            ranks.append(ds[f'top1_rank_{this_model}_late'].value_counts(normalize=True).sort_index().cumsum().index)\n",
    "            cum_props.append(ds[f'top1_rank_{this_model}_late'].value_counts(normalize=True).sort_index().cumsum().values)\n",
    "            concepts.append([concept]*ds[f'top1_rank_{this_model}_late'].value_counts(normalize=True).sort_index().cumsum().index.shape[0])\n",
    "            models.append([this_model]*ds[f'top1_rank_{this_model}_late'].value_counts(normalize=True).sort_index().cumsum().index.shape[0])\n",
    "            abstraction.append([this_abstraction]*ds[f'top1_rank_{this_model}_late'].value_counts(normalize=True).sort_index().cumsum().index.shape[0])\n",
    "\n",
    "    ranks = np.concatenate(ranks)\n",
    "    cum_props = np.concatenate(cum_props)\n",
    "    concepts = np.concatenate(concepts)\n",
    "    models = np.concatenate(models)\n",
    "    abstraction = np.concatenate(abstraction)\n",
    "\n",
    "    model_cum_prop_df = pd.DataFrame({'concept':concepts,'rank':ranks,'cum_prop':cum_props,'model':models,'abstraction':abstraction})\n",
    "\n",
    "    model_cum_prop_df['rank'] = model_cum_prop_df['rank']/1854\n",
    "\n",
    "    model_auc_df = pd.DataFrame(columns = ['concept','auc','abstraction','model'])\n",
    "\n",
    "    this_cum_prop_df = model_cum_prop_df[model_cum_prop_df['model']==this_model]\n",
    "\n",
    "    for this_concept in this_cum_prop_df.concept.unique():\n",
    "        for this_abstraction in cum_prop_df.abstraction.unique():\n",
    "            this_df = this_cum_prop_df[(this_cum_prop_df['concept']==this_concept) & (this_cum_prop_df['abstraction']==this_abstraction)]\n",
    "            this_df = pd.concat([this_df,pd.DataFrame({'concept':this_concept,'rank':1,'cum_prop':1, 'model':this_model,'abstraction':this_abstraction},index=[0])])\n",
    "            if this_df.shape[0]==1:\n",
    "                this_df = pd.concat([this_df,pd.DataFrame({'concept':this_concept,'rank':0,'cum_prop':1, 'model':this_model,'abstraction':this_abstraction},index=[0])])    \n",
    "            this_auc= this_df.groupby('concept').apply(lambda x: auc(x['rank'],x['cum_prop']))\n",
    "            model_auc_df = pd.concat([model_auc_df,pd.DataFrame({'concept':this_concept,'auc':this_auc,'abstraction':this_abstraction,'model':this_model})])\n",
    "\n",
    "    Y = this_auc_df['auc'].values\n",
    "    X = np.column_stack([model_auc_df['auc'].values,model_auc_df['abstraction'].values,model_auc_df['auc'].values*model_auc_df['abstraction'].values])\n",
    "    X = sm.add_constant(X)\n",
    "    m_snp = sm.OLS(Y,X).fit()\n",
    "\n",
    "    return m_snp.rsquared_adj\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in data csvs and preprocess them for analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set up directory structures\n",
    "\n",
    "proj_dir = os.path.abspath('../')\n",
    "data_dir = os.path.join(proj_dir, 'data')\n",
    "\n",
    "analysis_dir = os.path.join(proj_dir, 'analysis')\n",
    "results_dir = os.path.join(proj_dir, 'results')\n",
    "csv_dir = os.path.join(results_dir, 'csv')\n",
    "plot_dir = os.path.join(results_dir, 'plots')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "things_main_df = pd.read_csv(os.path.join(data_dir, 'things_concepts.tsv'), sep='\\t') ### main CSV of THINGS concept metadata\n",
    "human_recog_df = pd.read_csv(os.path.join(data_dir, 'human_recog_df.csv')) ### raw recognition experiment data on human sketches\n",
    "machine_recog_df = pd.read_csv(os.path.join(data_dir, 'machine_recog_df.csv')) ### raw recognition experiment data on machine sketches\n",
    "\n",
    "things1854concepts = things_main_df.uniqueID.values ## get a list of all the unique THINGS concept labels\n",
    "concept2category_dict = dict(zip(things_main_df.uniqueID, things_main_df['All Bottom-up Categories'])) ## create a dictionary of concepts to bottom-up categories\n",
    "\n",
    "\n",
    "spose_embeds = pd.read_csv(os.path.join(data_dir,'THINGS_spose.txt'), sep='\\t', header=None) ### semantic embeddings for things concepts based on human similarity ratings\n",
    "spose_cols = spose_embeds.columns.tolist()\n",
    "spose_embeds['concept']=things1854concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load in neural network class label probabilties for each sketch\n",
    "human_model_logits_dir = os.path.join(data_dir, '1854_human_model_logits')\n",
    "human_model_logits_files = [f for f in os.listdir(human_model_logits_dir) if f.endswith('.pkl')]\n",
    "\n",
    "excluded_models = ['things_df_human_metrics_neurips_mocov2_late.pkl','things_df_human_metrics_neurips_harm-rn_1.0.pkl'] ###models excluded for neurips DB 2023\n",
    "\n",
    "model_list = [f.split('_')[5] for f in human_model_logits_files if f not in excluded_models ] ### get a list of all models benchmarked\n",
    "\n",
    "machine_model_logits_dir = os.path.join(data_dir, '1854_machine_model_logits')\n",
    "machine_model_logits_files = [f for f in os.listdir(machine_model_logits_dir) if f.endswith('.pkl')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load in each model's data as keys in a dict\n",
    "model_logit_dict = defaultdict(dict)\n",
    "\n",
    "for f in human_model_logits_files:\n",
    "    if f not in excluded_models:\n",
    "        model_logit_dict[f.split('_')[5]]= pd.read_pickle(os.path.join(human_model_logits_dir, f))\n",
    "\n",
    "machine_model_logit_dict =  defaultdict(dict)\n",
    "for f in machine_model_logits_files:\n",
    "    if f not in excluded_models:\n",
    "        machine_model_logit_dict[f.split('_')[5]]= pd.read_pickle(os.path.join(machine_model_logits_dir, f))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## do minor preprocessing\n",
    "    \n",
    "human_recog_df['sketch_id']=human_recog_df.filename_recog.apply(lambda x: str(x).split('_')[-1].split('.')[0]) ### create new sketch_id column\n",
    "machine_recog_df['sketch_id']=machine_recog_df.filename_recog.apply(lambda x: str(x).split('.')[0]) ## create new sketch_id column\n",
    "\n",
    "human_recog_df = human_recog_df[human_recog_df['concept']!='cat'].reset_index(drop=True) ### remove practice trials (no cats in this benchmarking!)\n",
    "human_recog_df = human_recog_df[human_recog_df['concept']!='cat'].reset_index(drop=True)\n",
    "human_recog_df = human_recog_df[human_recog_df['sketch_id'].apply(lambda x: x!='nan')].reset_index(drop=True) ## remove certain trials where nans were recorded\n",
    "human_recog_df['response_list'] = human_recog_df['response'].apply(lambda x: list(ast.literal_eval(x).values())) ### create a list of all responses for a given sketch on any trial\n",
    "\n",
    "## repeat for recognition data on clipasso sketches\n",
    "machine_recog_df = machine_recog_df[machine_recog_df['concept']!='cat'].reset_index(drop=True) ### remove practice trials\n",
    "machine_recog_df = machine_recog_df[machine_recog_df['sketch_id'].apply(lambda x: x!='nan')].reset_index(drop=True)\n",
    "machine_recog_df['response_list'] =   machine_recog_df['response'].apply(lambda x: list(ast.literal_eval(x).values()))\n",
    "machine_recog_df['num_strokes'] = machine_recog_df['abstraction'] ### number of strokes == abstraction level for clipasso sketches\n",
    "\n",
    "\n",
    "\n",
    "## compute overall accuracy (if any response in list == ground truth)\n",
    "human_recog_df['correct'] = human_recog_df.apply(lambda x: x['uniqueID'] in (x['response_list']), axis=1)\n",
    "machine_recog_df['correct'] = machine_recog_df.apply(lambda x: x['uniqueID'] in (x['response_list']), axis=1)\n",
    "## compute top1 accuracy \n",
    "human_recog_df['top1_correct'] = human_recog_df.apply(lambda x: x['response_list'][0] == x['uniqueID'] if len(x['response_list'])>0 else False , axis=1)\n",
    "machine_recog_df['top1_correct'] = machine_recog_df.apply(lambda x: x['response_list'][0] == x['uniqueID'] if len(x['response_list'])>0 else False , axis=1)\n",
    "\n",
    "\n",
    "### remove weird sketch missing from models\n",
    "machine_recog_df = machine_recog_df[machine_recog_df.sketch_id!='canvas_09s_32']\n",
    "## remove any rows where response_list is empty\n",
    "human_recog_df = human_recog_df[human_recog_df['response_list'].apply(lambda x: len(x)>0)].reset_index(drop=True)\n",
    "machine_recog_df = machine_recog_df[machine_recog_df['response_list'].apply(lambda x: len(x)>0)].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### reshape data into a 8192 (sketches) x 1854 (concepts) matrix of counts for where the numbe in each cell is the number of times the label corresponding to that column\n",
    "### was assigned to each that sketch. Add some additional metadata to each row too including abstraction level, filename, number of stroeks, and concept\n",
    "human_response_vec_df = human_recog_df.explode('response_list').groupby(['filename_recog','filename','abstraction','sketch_id',\\\n",
    "                                                    'concept','uniqueID','num_strokes'])['response_list'].value_counts().unstack(fill_value=0).reset_index()\n",
    "### compute mean metrics for each sketch\n",
    "human_response_vec_df['mean_accuracy']=human_response_vec_df.apply(lambda x:human_recog_df[human_recog_df['filename_recog']==x.filename_recog].correct.mean(), axis=1)\n",
    "\n",
    "human_response_vec_df['mean_top1_accuracy']=human_response_vec_df.apply(lambda x:human_recog_df[human_recog_df['filename_recog']==x.filename_recog].top1_correct.mean(), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### repeat the process above for sketches generated by clipasso\n",
    "machine_response_vec_df = machine_recog_df.explode('response_list').groupby(['filename_recog','abstraction','sketch_id',\\\n",
    "                                                    'concept','uniqueID','num_strokes'])['response_list'].value_counts().unstack(fill_value=0).reset_index()\n",
    "machine_response_vec_df['mean_accuracy']=machine_response_vec_df.apply(lambda x:machine_recog_df[machine_recog_df['filename_recog']==x.filename_recog].correct.mean(), axis=1)\n",
    "\n",
    "machine_response_vec_df['mean_top1_accuracy']=machine_response_vec_df.apply(lambda x:machine_recog_df[machine_recog_df['filename_recog']==x.filename_recog].top1_correct.mean(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_response_vec_df = human_response_vec_df.rename_axis('id').reset_index(drop=True)\n",
    "machine_response_vec_df = machine_response_vec_df.rename_axis('id').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### fill in labels that were never used with 0s\n",
    "\n",
    "non_label_concepts = np.setdiff1d( things1854concepts, human_response_vec_df.columns)\n",
    "## add all the elemnts of non_label_concepts to human_response_vec_df and set them to 0\n",
    "for concept in non_label_concepts:\n",
    "    human_response_vec_df[concept]=0\n",
    "human_response_vec_df['abstraction'] = human_response_vec_df['abstraction'].apply(lambda x: int(x))\n",
    "human_response_vec_df = human_response_vec_df.sort_values(by=['filename_recog','filename','abstraction'])\n",
    "\n",
    "non_label_concepts = np.setdiff1d( things1854concepts,machine_response_vec_df.columns)\n",
    "\n",
    "for concept in non_label_concepts:\n",
    "    machine_response_vec_df[concept]=0\n",
    "machine_response_vec_df['abstraction'] = machine_response_vec_df['abstraction'].apply(lambda x: int(x))\n",
    "machine_response_vec_df = machine_response_vec_df.sort_values(by=['filename_recog','abstraction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### add a column to each df that is the entropy of the response vector for each sketch\n",
    "for i, row in human_response_vec_df.iterrows():\n",
    "    response_vec = np.array(row[things1854concepts.tolist()])\n",
    "    ### compute the entropy of the response vector\n",
    "    probabilities = response_vec / np.sum(response_vec)\n",
    "    entropy_value = entropy(probabilities.astype(float))\n",
    "    human_response_vec_df.loc[i,'entropy'] = entropy_value\n",
    "\n",
    "for i, row in machine_response_vec_df.iterrows():\n",
    "    response_vec = np.array(row[things1854concepts.tolist()])\n",
    "    ### compute the entropy of the response vector\n",
    "    probabilities = response_vec / np.sum(response_vec)\n",
    "    entropy_value = entropy(probabilities.astype(float))\n",
    "    machine_response_vec_df.loc[i,'entropy'] = entropy_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### create a cosine distrance matrix between each concept based on their semantic embeddings\n",
    "\n",
    "cosine_dist_df = pd.DataFrame(cosine_distances(spose_embeds[spose_cols]), columns=things1854concepts, index=things1854concepts)\n",
    "\n",
    "## for each concept rank the remaining concepts in terms of proximity in semantic embedding space\n",
    "nearest_concepts = {}\n",
    "for concept in cosine_dist_df.columns:\n",
    "    nearest_concepts[concept] = cosine_dist_df[concept].sort_values()[1:].index.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### for both human and clipasso sketches add the class probabilities and top1 guesses as columns to the respective dataframes\n",
    "sketch_id_order= {name: rank for rank, name in enumerate(human_response_vec_df.sketch_id.values)}\n",
    "\n",
    "for this_model in model_list:\n",
    "    this_df = model_logit_dict[this_model]\n",
    "    this_df_s = this_df[this_df.sketch_id.isin(human_response_vec_df.sketch_id.unique())]\n",
    "    this_df_s['sort_order']=this_df_s['sketch_id'].apply(lambda x: sketch_id_order[x])\n",
    "    this_df_s = this_df_s.sort_values(by=['sort_order'])\n",
    "    if this_model == 'harm-rn': ### add fix for harmonizing model name convention\n",
    "        human_response_vec_df[f'proba_{this_model}_late'] = this_df_s[f'proba_{this_model.split(\"-\")[0]}_rn_late'].values\n",
    "        human_response_vec_df[f'top1_{this_model}_late'] = this_df_s[f'top1_{this_model.split(\"-\")[0]}_rn_late'].values\n",
    "    elif this_model == 'harm-vit':\n",
    "        human_response_vec_df[f'proba_{this_model}_late'] = this_df_s[f'proba_{this_model.split(\"-\")[0]}_vit_late'].values\n",
    "        human_response_vec_df[f'top1_{this_model}_late'] = this_df_s[f'top1_{this_model.split(\"-\")[0]}_vit_late'].values\n",
    "    else:\n",
    "        human_response_vec_df[f'proba_{this_model}_late'] = this_df_s[f'proba_{this_model}_late'].values\n",
    "        human_response_vec_df[f'top1_{this_model}_late'] = this_df_s[f'top1_{this_model}_late'].values\n",
    "\n",
    "sketch_id_order= {name: rank for rank, name in enumerate(machine_response_vec_df.sketch_id.values)}\n",
    "for this_model in model_list:\n",
    "    if this_model not in ['ipcl','harm-vit','harm-rn','cornet','ecoset','simclr']:\n",
    "        this_df = machine_model_logit_dict[this_model]\n",
    "        this_df_s = this_df[this_df.sketch_id.isin(machine_response_vec_df.sketch_id.unique())]\n",
    "        this_df_s['sort_order']=this_df_s['sketch_id'].apply(lambda x: sketch_id_order[x])\n",
    "        this_df_s = this_df_s.sort_values(by=['sort_order'])\n",
    "        machine_response_vec_df[f'proba_{this_model}_late'] = this_df_s[f'proba_{this_model}_late'].values\n",
    "        machine_response_vec_df[f'top1_{this_model}_late'] = this_df_s[f'top1_{this_model}_late'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### for human-made sketches, get the rank of top-1 guess based on semantic embedding distance\n",
    "human_recog_df['top1_rank'] = \\\n",
    "    human_recog_df.apply(lambda x: 0 if x.response_list[0]==x.uniqueID else nearest_concepts[x.uniqueID].index(x.response_list[0]),axis=1)\n",
    "\n",
    "### for each model, get the top-1 guess and the rank of that guess based on semantic embedding distance\n",
    "for this_model in model_list:\n",
    "\n",
    "    human_response_vec_df[f'guess_{this_model}_late'] = [things1854concepts[np.argmax(x)] for x in human_response_vec_df[f'proba_{this_model}_late']]\n",
    "    human_response_vec_df[f'top1_rank_{this_model}_late'] = \\\n",
    "    human_response_vec_df.apply(lambda x: 0 if x[f'guess_{this_model}_late']==x.uniqueID else nearest_concepts[x.uniqueID].index(x[f'guess_{this_model}_late']),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### this chunk computes the semantic neigbhor preference metric\n",
    "### In short, for each sketch we compute how the cumulative proportion of guesses for that sketch varies as a function of rank.\n",
    "### If the guesses made by models and humans are semantically similar, then the cumulative proportion of guesses should increase more rapidly as a function of rank\n",
    "\n",
    "concepts=[]\n",
    "ranks = []\n",
    "cum_props=[]\n",
    "abstraction =[]\n",
    "\n",
    "for this_concept in human_response_vec_df.uniqueID.unique(): ## to maintain same order as response vec df\n",
    "    for this_abstraction in human_recog_df.abstraction.unique():\n",
    "\n",
    "        ds= human_recog_df[(human_recog_df['uniqueID']==this_concept) & (human_recog_df['abstraction']==this_abstraction)]\n",
    "        ds = ds[ds.top1_rank!=0]\n",
    "        ranks.append(ds['top1_rank'].value_counts(normalize=True).sort_index().cumsum().index)\n",
    "        cum_props.append(ds['top1_rank'].value_counts(normalize=True).sort_index().cumsum().values)\n",
    "        concepts.append([this_concept]*ds['top1_rank'].value_counts(normalize=True).sort_index().cumsum().index.shape[0])\n",
    "        abstraction.append([this_abstraction]*ds['top1_rank'].value_counts(normalize=True).sort_index().cumsum().index.shape[0])\n",
    "\n",
    "ranks = np.concatenate(ranks)\n",
    "cum_props = np.concatenate(cum_props)\n",
    "concepts = np.concatenate(concepts)\n",
    "abstraction = np.concatenate(abstraction)\n",
    "\n",
    "cum_prop_df = pd.DataFrame({'concept':concepts,'rank':ranks,'cum_prop':cum_props,'abstraction':abstraction})\n",
    "\n",
    "cum_prop_df['rank'] = cum_prop_df['rank']/1854\n",
    "\n",
    "\n",
    "concepts=[]\n",
    "ranks = []\n",
    "cum_props=[]\n",
    "models=[]\n",
    "abstraction =[]\n",
    "\n",
    "for concept in human_response_vec_df.uniqueID.unique():\n",
    "    for this_model in model_list:\n",
    "        for this_abstraction in human_response_vec_df.abstraction.unique():\n",
    "            ds= human_response_vec_df[(human_response_vec_df['uniqueID']==concept) &(human_response_vec_df['abstraction']==this_abstraction)]\n",
    "            ds = ds[ds[f'top1_rank_{this_model}_late']!=0]\n",
    "            ranks.append(ds[f'top1_rank_{this_model}_late'].value_counts(normalize=True).sort_index().cumsum().index)\n",
    "            cum_props.append(ds[f'top1_rank_{this_model}_late'].value_counts(normalize=True).sort_index().cumsum().values)\n",
    "            concepts.append([concept]*ds[f'top1_rank_{this_model}_late'].value_counts(normalize=True).sort_index().cumsum().index.shape[0])\n",
    "            models.append([this_model]*ds[f'top1_rank_{this_model}_late'].value_counts(normalize=True).sort_index().cumsum().index.shape[0])\n",
    "            abstraction.append([this_abstraction]*ds[f'top1_rank_{this_model}_late'].value_counts(normalize=True).sort_index().cumsum().index.shape[0])\n",
    "\n",
    "ranks = np.concatenate(ranks)\n",
    "cum_props = np.concatenate(cum_props)\n",
    "concepts = np.concatenate(concepts)\n",
    "models = np.concatenate(models)\n",
    "abstraction = np.concatenate(abstraction)\n",
    "\n",
    "model_cum_prop_df = pd.DataFrame({'concept':concepts,'rank':ranks,'cum_prop':cum_props,'model':models,'abstraction':abstraction})\n",
    "\n",
    "model_cum_prop_df['rank'] = model_cum_prop_df['rank']/1854\n",
    "\n",
    "### To compute the semantic neighbor preference we compute the area under the curve (AUC) of cumulative proportion of guesses vs. rank (normalized to be between 0-1)\n",
    "auc_df = pd.DataFrame(columns = ['concept','auc'])\n",
    "for this_concept in cum_prop_df.concept.unique():\n",
    "    for this_abstraction in cum_prop_df.abstraction.unique():\n",
    "        this_df = cum_prop_df[(cum_prop_df['concept']==this_concept) & (cum_prop_df['abstraction']==this_abstraction)]\n",
    "        this_df = pd.concat([this_df,pd.DataFrame({'concept':this_concept,'rank':1,'cum_prop':1,'abstraction':this_abstraction},index=[0])])\n",
    "        this_auc= this_df.groupby('concept').apply(lambda x: auc(x['rank'],x['cum_prop']))\n",
    "        auc_df = pd.concat([auc_df,pd.DataFrame({'concept':this_concept,'auc':this_auc,'abstraction':this_abstraction})])\n",
    "\n",
    "\n",
    "model_auc_df = pd.DataFrame(columns = ['concept','auc','abstraction','model'])\n",
    "\n",
    "for this_model in model_list:\n",
    "    this_cum_prop_df = model_cum_prop_df[model_cum_prop_df['model']==this_model]\n",
    "\n",
    "    for this_concept in this_cum_prop_df.concept.unique():\n",
    "        for this_abstraction in cum_prop_df.abstraction.unique():\n",
    "            this_df = this_cum_prop_df[(this_cum_prop_df['concept']==this_concept) & (this_cum_prop_df['abstraction']==this_abstraction)]\n",
    "            this_df = pd.concat([this_df,pd.DataFrame({'concept':this_concept,'rank':1,'cum_prop':1, 'model':this_model,'abstraction':this_abstraction},index=[0])])\n",
    "            if this_df.shape[0]==1:\n",
    "                this_df = pd.concat([this_df,pd.DataFrame({'concept':this_concept,'rank':0,'cum_prop':1, 'model':this_model,'abstraction':this_abstraction},index=[0])])    \n",
    "            this_auc= this_df.groupby('concept').apply(lambda x: auc(x['rank'],x['cum_prop']))\n",
    "            model_auc_df = pd.concat([model_auc_df,pd.DataFrame({'concept':this_concept,'auc':this_auc,'abstraction':this_abstraction,'model':this_model})])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots and analyses"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### human sketch understanding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Humans produce sparser sketches under stronger time constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from matplotlib.patches import Patch\n",
    "histcolors = sns.color_palette('magma_r', n_colors=len(human_response_vec_df['abstraction'].unique()))\n",
    "\n",
    "\n",
    "### taking the dataframe above make a histogram of num_strokes grouped by abstraction, make it a density plot\n",
    "sns.displot(data=human_response_vec_df, x='num_strokes', hue='abstraction', kind='kde',palette=histcolors, fill=True, legend=False,\\\n",
    "            height=5, aspect=2)\n",
    "\n",
    "mean_num_strokes = human_response_vec_df.groupby('abstraction')['num_strokes'].mean()\n",
    "\n",
    "# plot the vertical lines with the same colors as the density plot\n",
    "for i, abs_level in enumerate(mean_num_strokes.index):\n",
    "    plt.axvline(x=mean_num_strokes[abs_level], color=histcolors[i],linewidth=2)\n",
    "\n",
    "## show legend with the color of the boxes matching the order in histcolors\n",
    "plt.legend(handles=[Patch(color=histcolors[i], label=abs_level, alpha=.5) for i, abs_level in enumerate(mean_num_strokes.index)],fontsize=20)\n",
    "plt.xlabel('Number of strokes', fontsize=25)\n",
    "plt.ylabel('Density', fontsize=25)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "### increase the size of the legend markers\n",
    "\n",
    "plt.xlim(0,40)\n",
    "# plt.title('Distribution of stroke complexity by abstraction level (Human sketches)', fontsize=20)\n",
    "plt.savefig(os.path.join(plot_dir,'neuripsDB_stroke_complexity_by_dd.pdf'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sparser sketches are more semantically ambiguous for models and humans  +\n",
    "#### Different models display distinct patterns of sketch recognition behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab20 = sns.color_palette(\"tab20\", 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for this_model in model_list:\n",
    "    if this_model.startswith('harm'): ##fix for harmonizimg models\n",
    "        human_response_vec_df[f'H_{this_model}_late'] = human_response_vec_df[f'proba_{this_model.split(\"-\")[0]}-{this_model.split(\"-\")[1]}_late'].apply(lambda x: entropy(x))\n",
    "    else:\n",
    "        human_response_vec_df[f'H_{this_model}_late'] = human_response_vec_df[f'proba_{this_model}_late'].apply(lambda x: entropy(x))\n",
    "for this_model in model_list:\n",
    "    if this_model not in ['ipcl','harm-vit','harm-rn','cornet','ecoset','simclr']:\n",
    "        machine_response_vec_df[f'H_{this_model}_late'] = machine_response_vec_df[f'proba_{this_model}_late'].apply(lambda x: entropy(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.figure(figsize=(9,8))\n",
    "\n",
    "# sns.lineplot(data=human_response_vec_df, x='abstraction', y='mean_top1_accuracy',label='human',color=tab20[0],linewidth=3)\n",
    "for i,model in enumerate(model_list):\n",
    "    sns.lineplot(data=human_response_vec_df, x='abstraction', y=f'top1_{model}_late',label=model,color=tab20[i],linewidth=3)\n",
    "plt.xlabel('drawing time (s)',fontsize=30)\n",
    "plt.ylabel('top-1 classification accuracy (128-way)',fontsize=30)\n",
    "plt.xticks(fontsize=30)\n",
    "plt.yticks(fontsize=30)\n",
    "### increase the size of the legend text\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.,fontsize=20)\n",
    "plt.tight_layout()\n",
    "# plt.title('Human sketches',fontsize=25)\n",
    "plt.ylim(0,1)\n",
    "plt.axhline(y=(1/128), color='gray', linestyle='--',linewidth=2)\n",
    "plt.savefig(os.path.join(plot_dir,'neuripsDB_accuracy_v_dd.pdf'), dpi=300, bbox_inches='tight')\n",
    "### place the legend outside the figure/plot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.figure(figsize=(9,8))\n",
    "\n",
    "# sns.lineplot(data=human_response_vec_df, x='abstraction', y='mean_top1_accuracy',label='human',color=tab20[0],linewidth=3)\n",
    "for i,model in enumerate(model_list):\n",
    "    sns.lineplot(data=human_response_vec_df, x='abstraction', y=f'H_{model}_late',label=model,color=tab20[i],linewidth=3)\n",
    "plt.xlabel('drawing time (s)',fontsize=30)\n",
    "plt.ylabel('soft-label distribution entropy',fontsize=30)\n",
    "plt.xticks(fontsize=30)\n",
    "plt.yticks(fontsize=30)\n",
    "### increase the size of the legend text\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.,fontsize=20)\n",
    "plt.tight_layout()\n",
    "# plt.title('Human sketches',fontsize=25)\n",
    "plt.ylim(0,5)\n",
    "plt.axhline(y=entropy(np.ones(128)), color='gray', linestyle='--',linewidth=2)\n",
    "plt.savefig(os.path.join(plot_dir,'neuripsDB_entropy_v_dd.pdf'), dpi=300, bbox_inches='tight')\n",
    "### place the legend outside the figure/plot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_response_vec_df.groupby('abstraction')['mean_top1_accuracy','entropy'].agg(['mean','sem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.figure(figsize=(9,8))\n",
    "sns.lineplot(data=auc_df, x ='abstraction', y='auc',linewidth=4,alpha=.8,color='black')\n",
    "plt.xlabel('drawing time (ms)',fontsize=25)\n",
    "plt.ylabel('semantic neighbor preference',fontsize=25)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(15,8))\n",
    "sns.lineplot(data=model_auc_df, x ='abstraction', y='auc', hue='model',alpha=.6,palette=tab20,linewidth=3)\n",
    "# sns.lineplot(data=human_response_vec_df, x='abstraction', y='mean_top1_accuracy',label='human',color=tab20[0],linewidth=3)\n",
    "plt.ylim(.45,1)\n",
    "plt.xlabel('drawing time (s)',fontsize=30)\n",
    "plt.ylabel('semantic neighbor preference',fontsize=30)\n",
    "plt.xticks(fontsize=30)\n",
    "plt.yticks(fontsize=30)\n",
    "\n",
    "plt.axhline(y=.5, color='gray', linestyle='--',linewidth=2)\n",
    "### increase the size of the legend text\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.,fontsize=20)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(os.path.join(plot_dir,'neuripsDB_accuracy_v_snp.pdf'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_df.groupby('abstraction')['auc'].agg(['mean','sem'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A large gap remains between human and model sketch understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_acc_EVs_dict = {key: [] for key in model_list}\n",
    "model_H_EVs_dict = {key: [] for key in model_list}\n",
    "model_snp_EVs_dict = {key: [] for key in model_list}\n",
    "\n",
    "\n",
    "for i in range(2):\n",
    "    # Get unique conditions outside the loop\n",
    "    unique_conditions = human_response_vec_df.abstraction.unique()\n",
    "\n",
    "    # Create an empty list to collect filtered rows\n",
    "    filtered_rows = []\n",
    "    filtered_recog_rows = []\n",
    "\n",
    "    for concept in human_response_vec_df.concept.unique():\n",
    "        unique_ims = human_response_vec_df[human_response_vec_df.concept == concept].filename.unique()\n",
    "        # Sample 16 items from unique_ims with replacement\n",
    "        sample_ims = np.random.choice(unique_ims, 16, replace=True)\n",
    "        this_sample = list(itertools.product(sample_ims, unique_conditions))\n",
    "\n",
    "        for t in this_sample:\n",
    "            # Filter rows using boolean indexing\n",
    "            filtered_rows.append(human_response_vec_df[(human_response_vec_df.filename == t[0]) & (human_response_vec_df.abstraction == t[1])])\n",
    "            filtered_recog_rows.append(human_recog_df[(human_recog_df.filename == t[0]) & (human_recog_df.abstraction == t[1])])\n",
    "\n",
    "    # Concatenate all filtered rows once\n",
    "    this_sample_df = pd.concat(filtered_rows)\n",
    "    this_sample_recog_df = pd.concat(filtered_recog_rows)\n",
    "\n",
    "    ## aggregate by condition and concept\n",
    "    this_mean_df = this_sample_df.groupby(['concept','abstraction']).mean().reset_index()\n",
    "\n",
    "    this_auc_df = compute_human_snp(this_sample_recog_df)\n",
    "   \n",
    "    for this_model in model_list:\n",
    "        # acc_EVs=[]\n",
    "        # H_EVs=[]\n",
    "        # snp_EVs []\n",
    "        if this_model in ['harm-vit','harm-rn']:\n",
    "            this_mean_df= this_mean_df.rename(columns={f'top1_{this_model}_late':f'top1_{this_model.split(\"-\")[0]}_{this_model.split(\"-\")[1]}_late'})\n",
    "            this_mean_df= this_mean_df.rename(columns={f'H_{this_model}_late':f'H_{this_model.split(\"-\")[0]}_{this_model.split(\"-\")[1]}_late'})\n",
    "            m_acc = ols(f'mean_top1_accuracy~top1_{this_model.split(\"-\")[0]}_{this_model.split(\"-\")[1]}_late*abstraction',data=this_mean_df).fit()\n",
    "            m_H = ols(f'entropy~H_{this_model.split(\"-\")[0]}_{this_model.split(\"-\")[1]}_late*abstraction',data=this_mean_df).fit()\n",
    "        else:\n",
    "            ### fit a linear regression model to predict human accuracy from model accuracy\n",
    "            m_acc = ols(f'mean_top1_accuracy~top1_{this_model}_late*abstraction',data=this_mean_df).fit()\n",
    "            m_H = ols(f'entropy~H_{this_model}_late*abstraction',data=this_mean_df).fit()\n",
    "        # acc_EVs.append(m_acc.rsquared_adj)\n",
    "        # H_EVs.append(m_H.rsquared_adj)\n",
    "        model_acc_EVs_dict[this_model].append(m_acc.rsquared_adj)\n",
    "        model_H_EVs_dict[this_model].append(m_H.rsquared_adj)\n",
    "        model_snp_EVs_dict[this_model].append(compute_snp_EV(this_sample_df=this_sample_df, this_sample_recog_df=this_sample_recog_df,this_model=this_model,this_auc_df=this_auc_df))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "sh_recog_EVs_human=[]\n",
    "sh_entropy_EVs_human=[]\n",
    "sh_snp_EVs_human=[]\n",
    "\n",
    "all_subs = human_recog_df.subjectID.unique()\n",
    "for i in range(2):\n",
    "    random.shuffle(all_subs)\n",
    "    subset1 = all_subs[:all_subs.shape[0]//2]\n",
    "    subset2 = all_subs[all_subs.shape[0]//2:]\n",
    "\n",
    "    ###accuracy\n",
    "\n",
    "    subset1_df = human_recog_df[human_recog_df.subjectID.isin(subset1)].groupby(['filename','concept','abstraction']).mean().reset_index()\n",
    "    subset2_df = human_recog_df[human_recog_df.subjectID.isin(subset2)].groupby(['filename','concept','abstraction']).mean().reset_index()\n",
    "    \n",
    "    s1 = subset1_df.groupby(['concept','abstraction'])['top1_correct'].mean().reset_index()\n",
    "    s2 = subset2_df.groupby(['concept','abstraction'])['top1_correct'].mean().reset_index()\n",
    "    \n",
    "    Y = s1['top1_correct'].values\n",
    "    X = np.column_stack([s2['top1_correct'].values,s2['abstraction'].values,s2['top1_correct'].values*s2['abstraction'].values])\n",
    "    X = sm.add_constant(X)\n",
    "    m_acc = sm.OLS(Y,X).fit()\n",
    "\n",
    "    sh_recog_EVs_human.append(m_acc.rsquared_adj)\n",
    "\n",
    "    ###entropy\n",
    "\n",
    "\n",
    "    subset1_response_vec = human_recog_df[human_recog_df.subjectID.isin(subset1)].explode('response_list').groupby(['filename_recog','abstraction','sketch_id',\\\n",
    "                                                    'concept','uniqueID','num_strokes'])['response_list'].value_counts().unstack(fill_value=0).reset_index()\n",
    "    non_label_concepts = np.setdiff1d( things1854concepts, subset1_response_vec.columns)\n",
    "    for concept in non_label_concepts:\n",
    "        subset1_response_vec[concept]=0\n",
    "\n",
    "\n",
    "    subset2_response_vec = human_recog_df[human_recog_df.subjectID.isin(subset2)].explode('response_list').groupby(['filename_recog','abstraction','sketch_id',\\\n",
    "                                                    'concept','uniqueID','num_strokes'])['response_list'].value_counts().unstack(fill_value=0).reset_index()\n",
    "    non_label_concepts = np.setdiff1d( things1854concepts, subset2_response_vec.columns)\n",
    "    for concept in non_label_concepts:\n",
    "        subset2_response_vec[concept]=0\n",
    "\n",
    "\n",
    "    subset1_response_vec['entropy'] = subset1_response_vec[things1854concepts.tolist()].apply(lambda x: entropy(x), axis=1)\n",
    "    subset2_response_vec['entropy'] = subset2_response_vec[things1854concepts.tolist()].apply(lambda x: entropy(x), axis=1)\n",
    "    s1 = subset1_response_vec.groupby(['concept','abstraction'])['entropy'].mean().reset_index()\n",
    "    s2 = subset2_response_vec.groupby(['concept','abstraction'])['entropy'].mean().reset_index()\n",
    "\n",
    "    Y = s1['entropy'].values\n",
    "    X = np.column_stack([s2['entropy'].values,s2['abstraction'].values,s2['entropy'].values*s2['abstraction'].values])\n",
    "    X = sm.add_constant(X)\n",
    "\n",
    "    m_H = sm.OLS(Y,X).fit()\n",
    "   \n",
    "    sh_entropy_EVs_human.append(m_H.rsquared_adj)\n",
    "\n",
    "    ###snp\n",
    "\n",
    "    subset1_recog_df = human_recog_df[human_recog_df.subjectID.isin(subset1)]\n",
    "    subset2_recog_df = human_recog_df[human_recog_df.subjectID.isin(subset2)]\n",
    "\n",
    "    s1_auc_df = compute_human_snp(subset1_recog_df)\n",
    "    s2_auc_df = compute_human_snp(subset2_recog_df)\n",
    "\n",
    "    Y = s1_auc_df['auc'].values\n",
    "    X = np.column_stack([s2_auc_df['auc'].values,s2_auc_df['abstraction'].values,s2_auc_df['auc'].values*s2_auc_df['abstraction'].values])\n",
    "    X = sm.add_constant(X)\n",
    "\n",
    "    m_snp = sm.OLS(Y,X).fit()\n",
    "   \n",
    "    sh_snp_EVs_human.append(m_snp.rsquared_adj)\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##load the magma_r colormap\n",
    "bar_colors = sns.color_palette('magma_r', n_colors=3)\n",
    "\n",
    "human_acc_EVs=[]\n",
    "human_H_EVs=[]\n",
    "human_snp_EVs = []\n",
    "human_acc_EVs.append(np.mean(sh_recog_EVs_human))\n",
    "human_H_EVs.append(np.mean(sh_entropy_EVs_human))\n",
    "human_snp_EVs.append(np.mean(sh_snp_EVs_human))\n",
    "for this_model in model_list:\n",
    "\n",
    "    human_acc_EVs.append(np.mean(model_acc_EVs_dict[this_model]))\n",
    "    human_H_EVs.append(np.mean(model_H_EVs_dict[this_model]))\n",
    "    human_snp_EVs.append(np.mean(model_snp_EVs_dict[this_model]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "acc_df = pd.DataFrame({'model':np.insert(model_list, 0, 'reliability'),\\\n",
    "                       'EV':human_acc_EVs})\n",
    "H_df = pd.DataFrame({'model':np.insert(model_list, 0, 'reliability'),\\\n",
    "                          'EV':human_H_EVs})\n",
    "snp_df = pd.DataFrame({'model':np.insert(model_list, 0, 'reliability'),\\\n",
    "                            'EV':human_snp_EVs})\n",
    "\n",
    "\n",
    "acc_df = acc_df.sort_values(by=['EV','model'],ascending=False)\n",
    "H_df = H_df.sort_values(by=['EV','model'],ascending=False)\n",
    "snp_df = snp_df.sort_values(by=['EV','model'],ascending=False)\n",
    "\n",
    "\n",
    "\n",
    "tab20 = sns.color_palette('tab20',20)\n",
    "\n",
    "plt.figure(figsize=(8,7))\n",
    "# ax = sns.barplot(x='model',y='EV', data=acc_df, hue='agent',hue_order=['human','machine'], palette=[tab20[1],tab20[3]])\n",
    "ax = sns.barplot(x='model',y='EV', data=acc_df, color=bar_colors[0])\n",
    "\n",
    "bar_heights = [patch.get_height() for patch in ax.patches]\n",
    "\n",
    "errors = []\n",
    "errors.append(np.diff(np.percentile(sh_recog_EVs_human,[2.5,97.5])))\n",
    "for this_model in model_list:\n",
    "    errors.append(np.diff(np.percentile(model_acc_EVs_dict[this_model],[2.5,97.5]))[0])\n",
    "# err0 = np.diff(np.percentile(sh_recog_EVs_human,[2.5,97.5]))\n",
    "\n",
    "# Plot the error bars\n",
    "ax.errorbar(ax.get_xticks(), bar_heights, yerr=errors,color='black',linewidth=1,capsize=8,fmt='none')\n",
    "ax.patches[0].set_facecolor(bar_colors[1])\n",
    "# ax.patches[13].set_facecolor(tab20[2])\n",
    "plt.xlabel('',fontsize=25)\n",
    "plt.ylabel('human-model alignment (R^2)',fontsize=25)\n",
    "plt.xticks(fontsize=20, rotation=45)\n",
    "plt.yticks(fontsize=25)\n",
    "plt.ylim([0,1])\n",
    "plt.tight_layout()\n",
    "plt.title('classification accuracy',fontsize=25)\n",
    "# plt.legend(fontsize=20)\n",
    "plt.savefig(os.path.join(plot_dir,'neuripsDB_accuracy_alignment.pdf'), dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()\n",
    "#########\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8,7))\n",
    "# ax = sns.barplot(x='model',y='EV', data=acc_df, hue='agent',hue_order=['human','machine'], palette=[tab20[1],tab20[3]])\n",
    "ax = sns.barplot(x='model',y='EV', data=H_df, color=bar_colors[0])\n",
    "\n",
    "bar_heights = [patch.get_height() for patch in ax.patches]\n",
    "\n",
    "errors = []\n",
    "errors.append(np.diff(np.percentile(sh_entropy_EVs_human,[2.5,97.5])))\n",
    "for this_model in model_list:\n",
    "    errors.append(np.diff(np.percentile(model_H_EVs_dict[this_model],[2.5,97.5]))[0])\n",
    "# err0 = np.diff(np.percentile(sh_recog_EVs_human,[2.5,97.5]))\n",
    "\n",
    "# Plot the error bars\n",
    "ax.errorbar(ax.get_xticks(), bar_heights, yerr=errors,color='black',linewidth=1,capsize=8,fmt='none')\n",
    "ax.patches[0].set_facecolor(bar_colors[1])\n",
    "# ax.patches[13].set_facecolor(tab20[2])\n",
    "plt.xlabel('',fontsize=25)\n",
    "plt.ylabel('human-model alignment (R^2)',fontsize=25)\n",
    "plt.xticks(fontsize=20, rotation=45)\n",
    "plt.yticks(fontsize=25)\n",
    "plt.ylim([0,1])\n",
    "plt.tight_layout()\n",
    "plt.title('response entropy',fontsize=25)\n",
    "# plt.legend(fontsize=20)\n",
    "plt.savefig(os.path.join(plot_dir,'neuripsDB_entropy_alignment.pdf'), dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8,7))\n",
    "# ax = sns.barplot(x='model',y='EV', data=acc_df, hue='agent',hue_order=['human','machine'], palette=[tab20[1],tab20[3]])\n",
    "ax = sns.barplot(x='model',y='EV', data=snp_df, color=bar_colors[0])\n",
    "\n",
    "bar_heights = [patch.get_height() for patch in ax.patches]\n",
    "\n",
    "errors = []\n",
    "errors.append(np.diff(np.percentile(sh_snp_EVs_human,[2.5,97.5])))\n",
    "for this_model in model_list:\n",
    "    errors.append(np.diff(np.percentile(model_snp_EVs_dict[this_model],[2.5,97.5]))[0])\n",
    "# err0 = np.diff(np.percentile(sh_recog_EVs_human,[2.5,97.5]))\n",
    "\n",
    "# Plot the error bars\n",
    "ax.errorbar(ax.get_xticks(), bar_heights, yerr=errors,color='black',linewidth=1,capsize=8,fmt='none')\n",
    "ax.patches[0].set_facecolor(bar_colors[1])\n",
    "# ax.patches[13].set_facecolor(tab20[2])\n",
    "plt.xlabel('',fontsize=25)\n",
    "plt.ylabel('human-model alignment (R^2)',fontsize=25)\n",
    "plt.xticks(fontsize=20, rotation=45)\n",
    "plt.yticks(fontsize=25)\n",
    "plt.ylim([0,1])\n",
    "plt.tight_layout()\n",
    "plt.title('semantic neighbor preference',fontsize=25)\n",
    "\n",
    "plt.savefig(os.path.join(plot_dir,'neuripsDB_snp_alignment.pdf'), dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## t-tests\n",
    "\n",
    "model_recog_EVs=[]\n",
    "model_H_EVs =[]\n",
    "model_snp_EVs = []\n",
    "for model in model_list:\n",
    "    model_recog_EVs.extend(model_acc_EVs_dict[this_model])\n",
    "    model_H_EVs.extend(model_H_EVs_dict[this_model])\n",
    "    model_snp_EVs.extend(model_snp_EVs_dict[this_model])\n",
    "\n",
    "\n",
    "## do an independent samples t-test between model_recog_EVs and sh_recog_EVs_human\n",
    "\n",
    "print('acc',ttest_ind(sh_recog_EVs_human,model_recog_EVs,equal_var=False))\n",
    "print('H',ttest_ind(sh_entropy_EVs_human,model_H_EVs,equal_var=False))\n",
    "print('snp',ttest_ind(sh_snp_EVs_human,model_snp_EVs,equal_var=False))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "acc_order = acc_df.reset_index().rename(columns={'index': 'model_index'}).sort_values(by='model_index').index.values[1:]\n",
    "H_order = H_df.reset_index().rename(columns={'index': 'model_index'}).sort_values(by='model_index').index.values[1:]\n",
    "snp_order = snp_df.reset_index().rename(columns={'index': 'model_index'}).sort_values(by='model_index').index.values[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Combine rank sets into a 2D array\n",
    "rank_matrix = np.array([acc_order, H_order, snp_order])\n",
    "\n",
    "# Compute Spearman's rank correlation matrix\n",
    "correlation_matrix, p_values = spearmanr(rank_matrix, axis=1)\n",
    "\n",
    "print(\"Spearman's Rank Correlation Matrix:\")\n",
    "print(correlation_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,8))\n",
    "sns.heatmap(correlation_matrix, cmap='magma_r',annot=True,linewidth=.5,annot_kws={\"size\": 22,\"fontweight\":'bold'},xticklabels=['top-1 accuracy','entropy','SNP'],yticklabels=['top-1 accuracy','entropy','SNP'])\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "cbar = plt.gcf().axes[-1]\n",
    "cbar.tick_params(labelsize=18)\n",
    "plt.savefig(os.path.join(plot_dir,'neuripsDB_metrics_spearmanr.pdf'), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "###create output dataframe for human sketches for R\n",
    "\n",
    "human_sketches_R_df = human_response_vec_df[['sketch_id','abstraction','concept','num_strokes','entropy','mean_top1_accuracy']]\n",
    "human_sketches_R_df['model']='human'\n",
    "for this_model in model_list:\n",
    "    tdf = human_response_vec_df[['sketch_id','abstraction','concept','num_strokes', f'H_{this_model}_late', f'top1_{this_model}_late']]\n",
    "    tdf.columns = ['sketch_id','abstraction','concept','num_strokes', 'entropy', 'mean_top1_accuracy']\n",
    "    tdf['model']=this_model\n",
    "    human_sketches_R_df = pd.concat([human_sketches_R_df,tdf]).reset_index(drop=True)\n",
    "    \n",
    "\n",
    "human_sketches_R_df.to_csv(os.path.join(data_dir,'human_sketches_R_df.csv'),index=False)\n",
    "\n",
    "\n",
    "auc_df['model']='human'\n",
    "snp_R_df = pd.concat([auc_df,model_auc_df]).reset_index(drop=True)\n",
    "snp_R_df.to_csv(os.path.join(data_dir,'snp_R_df.csv'),index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLIPasso analyses"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A CLIP-based sketch generation algorithm emulates human sketches under some conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# human_response_vec_df.abstraction = human_response_vec_df.abstraction.astype(int)/1000\n",
    "plt.figure(figsize=(8,8))\n",
    "sns.pointplot(data=human_response_vec_df, x='abstraction', y='mean_top1_accuracy',label='human sketches',color=bar_colors[1],scale=1.5)\n",
    "sns.pointplot(data=machine_response_vec_df, x='abstraction', y='mean_top1_accuracy',label='CLIPasso sketches',color=bar_colors[0],scale=1.5)\n",
    "plt.xlabel('amount of detail',fontsize=25)\n",
    "plt.ylabel('top-1 classification accuracy (1,854-way)',fontsize=25)\n",
    "\n",
    "\n",
    "plt.xticks(fontsize=22)\n",
    "plt.yticks(fontsize=22)\n",
    "## change the x tick labels to be ['1','2','3','4']\n",
    "plt.gca().set_xticklabels(['1','2','3','4'])\n",
    "plt.legend(fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.ylim(0,.25)\n",
    "plt.savefig(os.path.join(plot_dir,'neuripsDB_human_CLIPasso_accuracy.pdf'), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLIPasso-human recognition accuracy vs. human-human recognition accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr(human_response_vec_df[~((human_response_vec_df.filename=='canvas_09s.jpg')&(human_response_vec_df.abstraction==32))].sort_values(['concept','abstraction'])['mean_top1_accuracy'],\\\n",
    "            machine_response_vec_df.sort_values(['concept','abstraction'])['mean_top1_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "human_machine_recog_df =pd.DataFrame(\n",
    "    {\n",
    "        \"human\": human_response_vec_df[~((human_response_vec_df.filename=='canvas_09s.jpg')&(human_response_vec_df.abstraction==32))].sort_values(['concept','abstraction'])['mean_top1_accuracy'].values,\n",
    "        \"machine\": machine_response_vec_df.sort_values(['concept','abstraction'])['mean_top1_accuracy'].values,\n",
    "        \"abstraction\": human_response_vec_df[~((human_response_vec_df.filename=='canvas_09s.jpg')&(human_response_vec_df.abstraction==32))].sort_values(['concept','abstraction'])['abstraction'].values,\n",
    "        \"concept\": human_response_vec_df[~((human_response_vec_df.filename=='canvas_09s.jpg')&(human_response_vec_df.abstraction==32))].sort_values(['concept','abstraction'])['concept'].values,\n",
    "\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "agg_human_machine_recog_df = human_machine_recog_df.groupby(['concept','abstraction']).mean().reset_index()\n",
    "md= smf.ols('human~machine*abstraction',data=agg_human_machine_recog_df).fit()\n",
    "print(md.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "human_grouped_responses = human_response_vec_df.groupby(['concept','abstraction'])[things1854concepts].mean().reset_index()\n",
    "machine_grouped_responses = machine_response_vec_df.groupby(['concept','abstraction'])[things1854concepts].mean().reset_index()\n",
    "response_concepts = np.repeat(human_grouped_responses.concept.unique(),4)\n",
    "concepts = []\n",
    "conds = []\n",
    "ranks = []\n",
    "\n",
    "for this_cond in  human_grouped_responses.abstraction.unique():\n",
    "    for this_concept in  human_grouped_responses.concept.unique():\n",
    "        this_machine_responses = machine_grouped_responses[(machine_grouped_responses.concept==this_concept)\\\n",
    "                                                           &(machine_grouped_responses.abstraction== this_cond)][things1854concepts].values\n",
    "        response_jsds = human_grouped_responses[things1854concepts].apply(lambda x: jsd(x, this_machine_responses[0]),axis=1)\n",
    "        jsd_ranks = rankdata(response_jsds.values)\n",
    "        this_concept_inds = [i for i, x in enumerate(response_concepts) if x == this_concept]\n",
    "        this_min_rank = np.min(jsd_ranks[this_concept_inds])\n",
    "        concepts.append(this_concept)\n",
    "        conds.append(this_cond)\n",
    "        ranks.append(this_min_rank)\n",
    "\n",
    "rank_df = pd.DataFrame({'concept': concepts, 'abstraction': conds, 'rank': ranks})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "sns.pointplot(data=rank_df, x='abstraction', y='rank',color=bar_colors[0],scale=1.5)\n",
    "plt.xlabel('amount of detail',fontsize=25)\n",
    "plt.ylabel('human-CLIPasso response divergence',fontsize=25)\n",
    "plt.gca().set_xticklabels(['1','2','3','4'])\n",
    "plt.xticks(fontsize=22)\n",
    "plt.yticks(fontsize=22)\n",
    "plt.ylim(0,40)\n",
    "plt.savefig(os.path.join(plot_dir,'neuripsDB_human_CLIPasso_divergence.pdf'), dpi=300, bbox_inches='tight')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##make rank_df['abstraction'] into a categorical variable\n",
    "# rank_df['abstraction'] = rank_df['abstraction'].astype('category')\n",
    "md= smf.ols('rank~abstraction',data=rank_df).fit()\n",
    "print(md.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sketch_models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "948ddb06426d110fb8709d965d8d26aeed62a2fade135e77a31f81eb95224669"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
